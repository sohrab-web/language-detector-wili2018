{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8296d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   0\n",
      "0  Label;English;Wiki Code;ISO 369-3;German;Langu...\n",
      "1   ace;Achinese;ace;ace;Achinesisch;Austronesian;;;\n",
      "2    afr;Afrikaans;af;afr;Afrikaans;Indo-European;;;\n",
      "3  als;Alemannic German;als;gsw;Alemannisch;Indo-...\n",
      "4       amh;Amharic;am;amh;Amharisch;Afro-Asiatic;;;\n",
      "Index([0], dtype='int64')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV without header\n",
    "label_map = pd.read_csv(\"labels.csv\", header=None)\n",
    "\n",
    "# Check what it looks like\n",
    "print(label_map.head())\n",
    "print(label_map.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "820c5607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Label           English Wiki Code ISO 369-3       German Language family  \\\n",
      "0   ace          Achinese       ace       ace  Achinesisch    Austronesian   \n",
      "1   afr         Afrikaans        af       afr    Afrikaans   Indo-European   \n",
      "2   als  Alemannic German       als       gsw  Alemannisch   Indo-European   \n",
      "3   amh           Amharic        am       amh    Amharisch    Afro-Asiatic   \n",
      "4   ang      Old English        ang       ang  Altenglisch   Indo-European   \n",
      "\n",
      "  Writing system                        Remarks        Synonyms  \n",
      "0            NaN                            NaN             NaN  \n",
      "1            NaN                            NaN             NaN  \n",
      "2            NaN  (ursprünglich nur Elsässisch)             NaN  \n",
      "3            NaN                            NaN             NaN  \n",
      "4            NaN                 (ca. 450-1100)  Angelsächsisch  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load CSV properly\n",
    "data = pd.read_csv(\"labels.csv\", sep=\";\", header=0)\n",
    "\n",
    "# Check the first rows\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a041e8c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Label', 'English', 'Wiki Code', 'ISO 369-3', 'German',\n",
      "       'Language family', 'Writing system', 'Remarks', 'Synonyms'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(data.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b52f26cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 117500\n",
      "Test samples: 117500\n",
      "First training example: Klement Gottwaldi surnukeha palsameeriti ning paigutati mausoleumi. Surnukeha oli aga liiga hilja ja oskamatult palsameeritud ning hakkas ilmutama lagunemise tundemärke. 1962. aastal viidi ta surnukeha mausoleumist ära ja kremeeriti. Zlíni linn kandis aastatel 1949–1989 nime Gottwaldov. Ukrainas Harkivi oblastis kandis Zmiivi linn aastatel 1976–1990 nime Gotvald. -> est\n"
     ]
    }
   ],
   "source": [
    "# Load training and test data\n",
    "with open(\"x_train.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    x_train = [line.strip() for line in f]\n",
    "\n",
    "with open(\"y_train.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    y_train = [line.strip() for line in f]  # keep as string (ISO code)\n",
    "\n",
    "with open(\"x_test.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    x_test = [line.strip() for line in f]\n",
    "\n",
    "with open(\"y_test.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    y_test = [line.strip() for line in f]  # keep as string\n",
    "\n",
    "# Quick check\n",
    "print(\"Training samples:\", len(x_train))\n",
    "print(\"Test samples:\", len(x_test))\n",
    "print(\"First training example:\", x_train[0], \"->\", y_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a670d1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ISO 'est' maps to: Estonian\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load label CSV\n",
    "label_map = pd.read_csv(\"labels.csv\", sep=\";\", header=0)\n",
    "\n",
    "# Create a dictionary: ISO code -> English language name\n",
    "iso_to_name = dict(zip(label_map[\"ISO 369-3\"], label_map[\"English\"]))\n",
    "\n",
    "# Quick test\n",
    "print(\"ISO 'est' maps to:\", iso_to_name[\"est\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67c5703f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training improved model...\n",
      "Training completed!\n",
      "Test Accuracy: 0.9262\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Improved vectorizer: captures more language patterns\n",
    "vectorizer = HashingVectorizer(\n",
    "    analyzer='char',\n",
    "    ngram_range=(1, 4),  # original working range\n",
    "    n_features=2**20,\n",
    "    alternate_sign=False\n",
    ")\n",
    "\n",
    "\n",
    "# Pipeline with Naive Bayes\n",
    "model = make_pipeline(vectorizer, MultinomialNB())\n",
    "\n",
    "# Train\n",
    "print(\"Training improved model...\")\n",
    "model.fit(x_train, y_train)\n",
    "print(\"Training completed!\")\n",
    "\n",
    "# Evaluate accuracy\n",
    "preds = model.predict(x_test)\n",
    "accuracy = (preds == y_test).mean()\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a304aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to detect language\n",
    "def detect_language(text):\n",
    "    iso_code = model.predict([text])[0]  # predict ISO code\n",
    "    return iso_to_name.get(iso_code, \"Unknown\")  # map to English name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "590a21a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from docx import Document\n",
    "import PyPDF2\n",
    "\n",
    "def read_document(file_path):\n",
    "    \"\"\"\n",
    "    Reads text from .txt, .docx, or .pdf files.\n",
    "    Returns the extracted text as a string.\n",
    "    \"\"\"\n",
    "    ext = os.path.splitext(file_path)[1].lower()\n",
    "\n",
    "    # TXT\n",
    "    if ext == \".txt\":\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return f.read()\n",
    "\n",
    "    # DOCX\n",
    "    elif ext == \".docx\":\n",
    "        doc = Document(file_path)\n",
    "        return \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "\n",
    "    # PDF\n",
    "    elif ext == \".pdf\":\n",
    "        reader = PyPDF2.PdfReader(file_path)\n",
    "        text = \"\"\n",
    "        for page in reader.pages:\n",
    "            extracted = page.extract_text()\n",
    "            if extracted:\n",
    "                text += extracted\n",
    "        return text\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format! Use .txt, .docx, or .pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30dbb0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User Provided Document: C:\\Users\\SU HAN\\Desktop\\urud1.txt\n",
      "Detected Language: urd\n"
     ]
    }
   ],
   "source": [
    "file_path = input(\"Enter file path: \").strip()\n",
    "\n",
    "# Read text from any supported file format\n",
    "text = read_document(file_path)\n",
    "\n",
    "# Predict language\n",
    "prediction = model.predict([text])[0]\n",
    "\n",
    "print(\"\\nUser Provided Document:\", file_path)\n",
    "print(\"Detected Language:\", prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f14856c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as 'language_detector.pkl'\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the trained model\n",
    "joblib.dump(model, \"language_detector.pkl\")\n",
    "print(\"Model saved as 'language_detector.pkl'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed25e31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Loading the trained model\n",
    "model = joblib.load(\"language_detector.pkl\")\n",
    "print(\"Model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bbc13f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User entered: quit \n",
      "Detected Language: Latin\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "# Make sure detect_language() is defined\n",
    "def detect_language(text):\n",
    "    iso_code = model.predict([text])[0]   # predict ISO code\n",
    "    return iso_to_name.get(iso_code, \"Unknown\")  # map to English name\n",
    "\n",
    "# Interactive testing\n",
    "while True:\n",
    "    user_input = input(\"Enter text (or 'quit' to exit): \")\n",
    "    if user_input.lower() == \"quit\":\n",
    "        break\n",
    "    \n",
    "    language_name = detect_language(user_input)\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    print(f\"User entered: {user_input}\")\n",
    "    print(f\"Detected Language: {language_name}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
